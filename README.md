# ğŸ“Š Benchmark de Engines de Processamento de Dados

## ğŸ¯ Sobre o Projeto

Este projeto implementa um sistema abrangente de benchmark para comparar a performance de diferentes engines de processamento de dados em Python: **DuckDB**, **Pandas** e **Polars**. O benchmark analisa operaÃ§Ãµes comuns de manipulaÃ§Ã£o de dados utilizando os microdados do ENEM 2023.

## ğŸ—ï¸ Arquitetura do Sistema

### Engines Testadas
- **ğŸ¦† DuckDB**: Engine SQL para analytics embarcada
- **ğŸ¼ Pandas**: Biblioteca tradicional para anÃ¡lise de dados em Python  
- **âš¡ Polars**: Engine moderna de DataFrames com foco em performance

### OperaÃ§Ãµes Benchmarkadas
1. **Leitura**: CSV e Parquet
2. **Filtros**: CondiÃ§Ãµes WHERE em colunas numÃ©ricas
3. **Joins**: OperaÃ§Ãµes de junÃ§Ã£o entre datasets
4. **AgregaÃ§Ãµes**: Agrupamentos e cÃ¡lculos estatÃ­sticos
5. **Escrita**: ExportaÃ§Ã£o para CSV e Parquet

### CenÃ¡rios de Teste
- **ğŸŸ¢ Pequeno**: Dataset reduzido para testes rÃ¡pidos
- **ğŸŸ¡ MÃ©dio**: Dataset intermediÃ¡rio para simulaÃ§Ã£o realÃ­stica  
- **ğŸ”´ Grande**: Dataset completo para teste de escalabilidade

## ğŸ“ Estrutura do Projeto

```
benchmark/
â”œâ”€â”€ main.py                           # Script principal de benchmark
â”œâ”€â”€ run_all_benchmarks.py            # ExecuÃ§Ã£o automatizada de todos os cenÃ¡rios
â”œâ”€â”€ requirements.txt                  # DependÃªncias do projeto
â”œâ”€â”€ pyproject.toml                   # ConfiguraÃ§Ãµes do Ruff (linter/formatter)
â”œâ”€â”€ README.md                        # Este arquivo
â”œâ”€â”€ CONSOLIDACAO_README.md           # DocumentaÃ§Ã£o do sistema de consolidaÃ§Ã£o
â”œâ”€â”€ README_PARAMETRIZACAO.md         # DocumentaÃ§Ã£o da parametrizaÃ§Ã£o
â”œâ”€â”€ RUFF_USAGE.md                    # Guia de uso do Ruff
â”œâ”€â”€ microdados_enem_2023/            # Dataset do ENEM 2023
â”‚   â”œâ”€â”€ DADOS/
â”‚   â”‚   â”œâ”€â”€ MICRODADOS_ENEM_2023.csv           # Dataset grande (latin-1)
â”‚   â”‚   â”œâ”€â”€ MICRODADOS_ENEM_2023_medio.csv     # Dataset mÃ©dio (utf-8)
â”‚   â”‚   â”œâ”€â”€ MICRODADOS_ENEM_2023_pequeno.csv   # Dataset pequeno (utf-8)
â”‚   â”‚   â”œâ”€â”€ *.parquet                          # VersÃµes em Parquet
â”‚   â”‚   â””â”€â”€ ITENS_PROVA_2023.csv              # Dados auxiliares para joins
â”‚   â”œâ”€â”€ DICIONÃRIO/                           # DocumentaÃ§Ã£o dos dados
â”‚   â”œâ”€â”€ PROVAS E GABARITOS/                   # Material adicional
â”‚   â””â”€â”€ ...
â”œâ”€â”€ benchmark_resultados_*.csv       # Resultados individuais por cenÃ¡rio
â”œâ”€â”€ resultados_geral.csv            # Resultados consolidados
â””â”€â”€ output.*                        # Arquivos temporÃ¡rios gerados pelos testes
```

## ğŸš€ Como Usar

### PrÃ©-requisitos
```bash
# Python 3.8+
python --version

# Instalar dependÃªncias
pip install -r requirements.txt
```

### ExecuÃ§Ã£o RÃ¡pida
```bash
# Executar um cenÃ¡rio especÃ­fico
python main.py pequeno   # ~2-5 minutos
python main.py medio     # ~5-15 minutos  
python main.py grande    # ~15-60 minutos

# Executar todos os cenÃ¡rios automaticamente
python run_all_benchmarks.py
```

### ParÃ¢metros de ConfiguraÃ§Ã£o

No arquivo `main.py`, vocÃª pode ajustar:
```python
TEST_REPEATS = 10  # NÃºmero de repetiÃ§Ãµes para cada teste (padrÃ£o: 10)
```

## ğŸ“Š Resultados e AnÃ¡lise

### Arquivos Gerados

1. **Resultados Individuais**:
   - `benchmark_resultados_pequeno.csv`
   - `benchmark_resultados_medio.csv`
   - `benchmark_resultados_grande.csv`

2. **Resultado Consolidado**:
   - `resultados_geral.csv` - Todos os resultados em um arquivo Ãºnico

### MÃ©tricas Coletadas

- **â±ï¸ Tempo de ExecuÃ§Ã£o**: Tempo em segundos para cada operaÃ§Ã£o
- **ğŸ§  Uso de MemÃ³ria**: Pico de memÃ³ria RAM utilizada em MB
- **ğŸ“ˆ EstatÃ­sticas**: MÃ©dias, desvios padrÃ£o e anÃ¡lises comparativas

### Estrutura dos Resultados
```csv
engine,operation,scenario,time_seconds,memory_mb
duckdb,read_csv,pequeno,1.234,45.2
pandas,read_csv,pequeno,2.456,78.9
polars,read_csv,pequeno,0.987,32.1
...
```

## ğŸ”§ Funcionalidades AvanÃ§adas

### 1. Sistema de Encoding Inteligente
- **Pequeno/MÃ©dio**: UTF-8 encoding
- **Grande**: Latin-1 encoding (para compatibilidade com dados oficiais)

### 2. ConsolidaÃ§Ã£o AutomÃ¡tica
- Combina resultados de mÃºltiplas execuÃ§Ãµes
- Gera estatÃ­sticas resumidas automaticamente
- Detecta e processa arquivos existentes

### 3. Monitoramento de Recursos
- Limpeza automÃ¡tica de garbage collector
- MediÃ§Ã£o precisa do uso de memÃ³ria
- EstabilizaÃ§Ã£o entre mediÃ§Ãµes

### 4. Tratamento de Erros
- Captura e registra erros por engine/operaÃ§Ã£o
- Continua execuÃ§Ã£o mesmo com falhas individuais
- Logs detalhados para debugging

## ğŸ“ˆ Casos de Uso

### Para Pesquisadores
- Comparar performance de diferentes engines
- Analisar escalabilidade com tamanhos de dados crescentes
- Identificar gargalos em operaÃ§Ãµes especÃ­ficas

### Para Desenvolvedores
- Escolher a melhor engine para diferentes cenÃ¡rios
- Otimizar pipelines de processamento de dados
- Validar performance antes de mudanÃ§as de arquitetura

### Para Analistas de Dados
- Entender trade-offs entre facilidade de uso e performance
- Planejar recursos computacionais para projetos
- Otimizar workflows de anÃ¡lise

## ğŸ› ï¸ Tecnologias Utilizadas

### Core Dependencies
- **DuckDB** `1.3.1` - Engine SQL embarcada
- **Pandas** `2.3.0` - ManipulaÃ§Ã£o de dados tradicional
- **Polars** `1.30.0` - Engine moderna de DataFrames
- **PyArrow** `20.0.0` - Backend para formatos colunares

### Utilities
- **psutil** `7.0.0` - Monitoramento de sistema
- **memory-profiler** `0.61.0` - Profiling de memÃ³ria
- **Ruff** `â‰¥0.1.0` - Linter e formatter Python

## ğŸ“‹ Metodologia de Benchmark

### Processo de MediÃ§Ã£o
1. **PreparaÃ§Ã£o**: Limpeza de garbage collector (3x)
2. **EstabilizaÃ§Ã£o**: Pausa para estabilizar memÃ³ria
3. **MediÃ§Ã£o Inicial**: MÃºltiplas leituras de memÃ³ria base
4. **ExecuÃ§Ã£o**: OperaÃ§Ã£o sendo benchmarkada
5. **MediÃ§Ã£o Final**: Captura do pico de memÃ³ria
6. **Limpeza**: Garbage collection e liberaÃ§Ã£o de recursos

### Controles de Qualidade
- MÃºltiplas repetiÃ§Ãµes por teste (padrÃ£o: 10)
- MediÃ§Ãµes de memÃ³ria com mÃºltiplas amostras
- Uso do menor valor de memÃ³ria inicial para precisÃ£o
- ForÃ§a coleta de garbage entre mediÃ§Ãµes

## ğŸ¤ Contribuindo

### Desenvolvimento Local
```bash
# Configurar ambiente
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate     # Windows

pip install -r requirements.txt

# Executar linter/formatter
ruff check .              # Verificar cÃ³digo
ruff format .             # Formatar cÃ³digo
```

### Adicionando Novas Engines
1. Implementar funÃ§Ãµes seguindo o padrÃ£o: `engine_operation(path, encoding="utf-8")`
2. Adicionar Ã  lista de engines na seÃ§Ã£o de execuÃ§Ã£o
3. Testar com diferentes cenÃ¡rios

### Adicionando Novas OperaÃ§Ãµes
1. Implementar para todas as engines existentes
2. Adicionar ao mapeamento de operaÃ§Ãµes
3. Documentar a nova funcionalidade

## ğŸ“œ LicenÃ§a

Este projeto Ã© desenvolvido para fins acadÃªmicos e de pesquisa. Os dados do ENEM sÃ£o de domÃ­nio pÃºblico, disponibilizados pelo INEP.

## ğŸ“ Suporte

Para dÃºvidas, sugestÃµes ou problemas:
- Abra uma issue no repositÃ³rio
- Consulte a documentaÃ§Ã£o adicional em `/docs/`
- Verifique os logs de execuÃ§Ã£o para debugging

---

**ğŸ’¡ Dica**: Execute primeiro o cenÃ¡rio "pequeno" para validar a configuraÃ§Ã£o antes de rodar benchmarks completos nos datasets maiores.

**âš ï¸ AtenÃ§Ã£o**: O cenÃ¡rio "grande" pode consumir recursos significativos de CPU e memÃ³ria. Monitore o sistema durante a execuÃ§Ã£o.